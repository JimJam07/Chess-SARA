{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d0c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from src.utils.model import ChessEngine\n",
    "import src.utils.board_to_tensor as bt\n",
    "from src.utils.data_preperation import chessDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baee5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_file = \"dataset/board_1_tensors.pt\"\n",
    "batch_size = 256\n",
    "dataloader = chessDataLoader(tensor_file, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2483d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, num_epochs=10, device='mps'):\n",
    "    if not os.path.exists(\"trainer\"):\n",
    "        os.makedirs(\"trainer\")\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "\n",
    "            loss = criterion(outputs, batch[:, -1, :, :, :].view(-1, 4672))  # Adjust as needed\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        torch.save(model.state_dict(), f\"trainer/model_epoch_{epoch+1}.pth\")\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    # Plot loss curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, marker='o', label=\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"trainer/training_metrics.png\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa107e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                     | 0/12702 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 4672]' is invalid for input of size 311296",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m train(model, train_loader, criterion, optimizer, num_epochs, device)\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :, :]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4672\u001b[39m))  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 4672]' is invalid for input of size 311296"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 256\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "    device = \"mps\"\n",
    "\n",
    "    # Load dataset\n",
    "    train_loader = chessDataLoader(\"dataset/board_1_tensors.pt\", batch_size=batch_size)\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = ChessEngine().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train model\n",
    "    train(model, train_loader, criterion, optimizer, num_epochs, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
